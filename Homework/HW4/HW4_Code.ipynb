{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4 - Developing Neural Network Architectures\n",
    "\n",
    "**Author**: Ewan Lister\n",
    "Completed 05/08/2023\n",
    "\n",
    "In this notebook we will use neural networks create fits for numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Reconsider the data from homework one:\n",
    "\n",
    "    X=np.arange(0,31)\n",
    "    Y=np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41,\n",
    "    40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# imports\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.io import loadmat\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X = torch.arange(0, 31, dtype=torch.float32).reshape(-1, 1)\n",
    "Y = torch.tensor([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41,\n",
    "                  40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53],\n",
    "                 dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "data = dict(zip(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Fit the data to a three layer feed forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [31/31], Loss: 1379.1917\n",
      "Epoch [2/15], Step [31/31], Loss: 589.0872\n",
      "Epoch [3/15], Step [31/31], Loss: 302.5200\n",
      "Epoch [4/15], Step [31/31], Loss: 188.1364\n",
      "Epoch [5/15], Step [31/31], Loss: 138.0802\n",
      "Epoch [6/15], Step [31/31], Loss: 114.4907\n",
      "Epoch [7/15], Step [31/31], Loss: 102.7859\n",
      "Epoch [8/15], Step [31/31], Loss: 96.7878\n",
      "Epoch [9/15], Step [31/31], Loss: 93.6552\n",
      "Epoch [10/15], Step [31/31], Loss: 92.0017\n",
      "Epoch [11/15], Step [31/31], Loss: 91.1240\n",
      "Epoch [12/15], Step [31/31], Loss: 90.6564\n",
      "Epoch [13/15], Step [31/31], Loss: 90.4069\n",
      "Epoch [14/15], Step [31/31], Loss: 90.2738\n",
      "Epoch [15/15], Step [31/31], Loss: 90.2027\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "class ThreeLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 20)  # input layer -> hidden layer\n",
    "        self.fc2 = nn.Linear(20, 10) # hidden layer -> hidden layer\n",
    "        self.fc3 = nn.Linear(10, 1)  # hidden layer -> output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize network\n",
    "net = ThreeLayerNet()\n",
    "\n",
    "# make a forward pass with dummy data\n",
    "x = torch.randn(1)\n",
    "output = net(x)\n",
    "\n",
    "# create a visualization\n",
    "vis_graph = make_dot(output, params=dict(net.named_parameters()))\n",
    "vis_graph.view()\n",
    "\n",
    "# use SGD for fitting\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# create training data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=data, batch_size=1, shuffle=True)\n",
    "\n",
    "# Train the neural network using gradient descent\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x) in enumerate(X):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(x)\n",
    "        loss = criterion(outputs, Y[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 31 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, 31, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Using the first 20 data points as training data, fit the neural network. Compute the least-square error for each of these over the training points. Then compute the least square error of these models on the test data which are the remaining 10 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [20/20], Loss: 4.1679\n",
      "Epoch [2/15], Step [20/20], Loss: 10.2078\n",
      "Epoch [3/15], Step [20/20], Loss: 15.7213\n",
      "Epoch [4/15], Step [20/20], Loss: 20.0622\n",
      "Epoch [5/15], Step [20/20], Loss: 23.2546\n",
      "Epoch [6/15], Step [20/20], Loss: 25.5169\n",
      "Epoch [7/15], Step [20/20], Loss: 27.0857\n",
      "Epoch [8/15], Step [20/20], Loss: 28.1591\n",
      "Epoch [9/15], Step [20/20], Loss: 28.8874\n",
      "Epoch [10/15], Step [20/20], Loss: 29.3787\n",
      "Epoch [11/15], Step [20/20], Loss: 29.7090\n",
      "Epoch [12/15], Step [20/20], Loss: 29.9306\n",
      "Epoch [13/15], Step [20/20], Loss: 30.0789\n",
      "Epoch [14/15], Step [20/20], Loss: 30.1782\n",
      "Epoch [15/15], Step [20/20], Loss: 30.2446\n",
      "Train error for x = tensor([0.]), y = tensor([30.]) : 74.1404\n",
      "Train error for x = tensor([1.]), y = tensor([35.]) : 13.0356\n",
      "Train error for x = tensor([2.]), y = tensor([33.]) : 31.4775\n",
      "Train error for x = tensor([3.]), y = tensor([32.]) : 43.6985\n",
      "Train error for x = tensor([4.]), y = tensor([34.]) : 21.2565\n",
      "Train error for x = tensor([5.]), y = tensor([37.]) : 2.5936\n",
      "Train error for x = tensor([6.]), y = tensor([39.]) : 0.1517\n",
      "Train error for x = tensor([7.]), y = tensor([38.]) : 0.3727\n",
      "Train error for x = tensor([8.]), y = tensor([36.]) : 6.8146\n",
      "Train error for x = tensor([9.]), y = tensor([36.]) : 6.8146\n",
      "Train error for x = tensor([10.]), y = tensor([37.]) : 2.5936\n",
      "Train error for x = tensor([11.]), y = tensor([39.]) : 0.1517\n",
      "Train error for x = tensor([12.]), y = tensor([42.]) : 11.4888\n",
      "Train error for x = tensor([13.]), y = tensor([45.]) : 40.8260\n",
      "Train error for x = tensor([14.]), y = tensor([45.]) : 40.8260\n",
      "Train error for x = tensor([15.]), y = tensor([41.]) : 5.7098\n",
      "Train error for x = tensor([16.]), y = tensor([40.]) : 1.9308\n",
      "Train error for x = tensor([17.]), y = tensor([39.]) : 0.1517\n",
      "Train error for x = tensor([18.]), y = tensor([42.]) : 11.4888\n",
      "Train error for x = tensor([19.]), y = tensor([44.]) : 29.0469\n",
      "\n",
      "\n",
      "Test error for x = tensor([20.]), y = tensor([47.]) : 70.3840\n",
      "Test error for x = tensor([21.]), y = tensor([49.]) : 107.9421\n",
      "Test error for x = tensor([22.]), y = tensor([50.]) : 129.7211\n",
      "Test error for x = tensor([23.]), y = tensor([49.]) : 107.9421\n",
      "Test error for x = tensor([24.]), y = tensor([46.]) : 54.6050\n",
      "Test error for x = tensor([25.]), y = tensor([48.]) : 88.1631\n",
      "Test error for x = tensor([26.]), y = tensor([50.]) : 129.7211\n",
      "Test error for x = tensor([27.]), y = tensor([53.]) : 207.0582\n",
      "Test error for x = tensor([28.]), y = tensor([55.]) : 268.6163\n",
      "Test error for x = tensor([29.]), y = tensor([54.]) : 236.8373\n",
      "Test error for x = tensor([30.]), y = tensor([53.]) : 207.0582\n"
     ]
    }
   ],
   "source": [
    "def check_train_test_error(x_train, y_train, x_test, y_test):\n",
    "    for i, (x) in enumerate(x_train):\n",
    "        outputs = net(x)\n",
    "        error = criterion(outputs, y_train[i])\n",
    "        print('Train error for x = {}, y = {} : {:.4f}'.format(x, y_train[i], error))\n",
    "    print('\\n')\n",
    "    for i, (x) in enumerate(x_test):\n",
    "        outputs = net(x)\n",
    "        error = criterion(outputs, y_test[i])\n",
    "        print('Test error for x = {}, y = {} : {:.4f}'.format(x, y_test[i], error))\n",
    "\n",
    "# isolate first 20 data points\n",
    "x_train = X[0:20]\n",
    "y_train = Y[0:20]\n",
    "x_test = X[20:31]\n",
    "y_test = Y[20:31]\n",
    "\n",
    "# train network on first 20 data points, examine progress of SGD via print statements\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x) in enumerate(x_train):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(x)\n",
    "        loss = criterion(outputs, y_train[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, 20, loss.item()))\n",
    "\n",
    "check_train_test_error(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Repeat (iii) but use the first 10 and last 10 data points as training data. Then fit the model to the test data (which are the 10 held out middle data points). Compare these results to (iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [20/20], Loss: 197.9372\n",
      "Epoch [2/50], Step [20/20], Loss: 163.9305\n",
      "Epoch [3/50], Step [20/20], Loss: 143.4147\n",
      "Epoch [4/50], Step [20/20], Loss: 130.7341\n",
      "Epoch [5/50], Step [20/20], Loss: 122.7551\n",
      "Epoch [6/50], Step [20/20], Loss: 117.6710\n",
      "Epoch [7/50], Step [20/20], Loss: 114.4027\n",
      "Epoch [8/50], Step [20/20], Loss: 112.2894\n",
      "Epoch [9/50], Step [20/20], Loss: 110.9174\n",
      "Epoch [10/50], Step [20/20], Loss: 110.0242\n",
      "Epoch [11/50], Step [20/20], Loss: 109.4417\n",
      "Epoch [12/50], Step [20/20], Loss: 109.0614\n",
      "Epoch [13/50], Step [20/20], Loss: 108.8132\n",
      "Epoch [14/50], Step [20/20], Loss: 108.6509\n",
      "Epoch [15/50], Step [20/20], Loss: 108.5447\n",
      "Epoch [16/50], Step [20/20], Loss: 108.4753\n",
      "Epoch [17/50], Step [20/20], Loss: 108.4300\n",
      "Epoch [18/50], Step [20/20], Loss: 108.4004\n",
      "Epoch [19/50], Step [20/20], Loss: 108.3809\n",
      "Epoch [20/50], Step [20/20], Loss: 108.3682\n",
      "Epoch [21/50], Step [20/20], Loss: 108.3599\n",
      "Epoch [22/50], Step [20/20], Loss: 108.3545\n",
      "Epoch [23/50], Step [20/20], Loss: 108.3508\n",
      "Epoch [24/50], Step [20/20], Loss: 108.3485\n",
      "Epoch [25/50], Step [20/20], Loss: 108.3469\n",
      "Epoch [26/50], Step [20/20], Loss: 108.3461\n",
      "Epoch [27/50], Step [20/20], Loss: 108.3455\n",
      "Epoch [28/50], Step [20/20], Loss: 108.3450\n",
      "Epoch [29/50], Step [20/20], Loss: 108.3446\n",
      "Epoch [30/50], Step [20/20], Loss: 108.3444\n",
      "Epoch [31/50], Step [20/20], Loss: 108.3442\n",
      "Epoch [32/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [33/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [34/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [35/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [36/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [37/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [38/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [39/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [40/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [41/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [42/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [43/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [44/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [45/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [46/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [47/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [48/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [49/50], Step [20/20], Loss: 108.3441\n",
      "Epoch [50/50], Step [20/20], Loss: 108.3441\n",
      "Train error for x = tensor([0.]), y = tensor([30.]) : 195.5338\n",
      "Train error for x = tensor([1.]), y = tensor([35.]) : 80.7004\n",
      "Train error for x = tensor([2.]), y = tensor([33.]) : 120.6338\n",
      "Train error for x = tensor([3.]), y = tensor([32.]) : 143.6005\n",
      "Train error for x = tensor([4.]), y = tensor([34.]) : 99.6671\n",
      "Train error for x = tensor([5.]), y = tensor([37.]) : 48.7671\n",
      "Train error for x = tensor([6.]), y = tensor([39.]) : 24.8337\n",
      "Train error for x = tensor([7.]), y = tensor([38.]) : 35.8004\n",
      "Train error for x = tensor([8.]), y = tensor([36.]) : 63.7337\n",
      "Train error for x = tensor([9.]), y = tensor([36.]) : 63.7337\n",
      "Train error for x = tensor([20.]), y = tensor([47.]) : 9.1002\n",
      "Train error for x = tensor([21.]), y = tensor([49.]) : 25.1669\n",
      "Train error for x = tensor([22.]), y = tensor([50.]) : 36.2002\n",
      "Train error for x = tensor([23.]), y = tensor([49.]) : 25.1669\n",
      "Train error for x = tensor([24.]), y = tensor([46.]) : 4.0669\n",
      "Train error for x = tensor([25.]), y = tensor([48.]) : 16.1335\n",
      "Train error for x = tensor([26.]), y = tensor([50.]) : 36.2002\n",
      "Train error for x = tensor([27.]), y = tensor([53.]) : 81.3001\n",
      "Train error for x = tensor([28.]), y = tensor([55.]) : 121.3668\n",
      "Train error for x = tensor([29.]), y = tensor([54.]) : 100.3335\n",
      "Train error for x = tensor([30.]), y = tensor([53.]) : 81.3001\n",
      "\n",
      "\n",
      "Test error for x = tensor([10.]), y = tensor([37.]) : 48.7671\n",
      "Test error for x = tensor([11.]), y = tensor([39.]) : 24.8337\n",
      "Test error for x = tensor([12.]), y = tensor([42.]) : 3.9336\n",
      "Test error for x = tensor([13.]), y = tensor([45.]) : 1.0336\n",
      "Test error for x = tensor([14.]), y = tensor([45.]) : 1.0336\n",
      "Test error for x = tensor([15.]), y = tensor([41.]) : 8.9003\n",
      "Test error for x = tensor([16.]), y = tensor([40.]) : 15.8670\n",
      "Test error for x = tensor([17.]), y = tensor([39.]) : 24.8337\n",
      "Test error for x = tensor([18.]), y = tensor([42.]) : 3.9336\n",
      "Test error for x = tensor([19.]), y = tensor([44.]) : 0.0003\n"
     ]
    }
   ],
   "source": [
    "# isolate first and last 10 training points\n",
    "x_train = torch.cat([X[0:10], X[20:31]])\n",
    "y_train = torch.cat([Y[0:10], Y[20:31]])\n",
    "x_test = X[10:20]\n",
    "y_test = Y[10:20]\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# train network on first and last 10 data points, examine progress of SGD via print statements\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x) in enumerate(x_train):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(x)\n",
    "        loss = criterion(outputs, y_train[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, 20, loss.item()))\n",
    "\n",
    "check_train_test_error(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) Compare the models fit in homework one to the neural networks in (ii) and (iii)\n",
    "\n",
    "Similarly to the curve fittin in homework 1, the neural network does a poor job of making any extrapolations about its test data if the data is outside of the domain of the training data. For example, the network did well when test data contained the 10 points between point 9 and point 20, but poorly when the test data was that from 20 to 31, which is unbounded by any training data. Thus is performs very similarly to the curve fitting in homework 1. However, the loss, for each value is still much greater in the case of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## II Now train a feedforward neural network on the MNIST data set. You will start by performing the following analysis:\n",
    "\n",
    "### (i) Compute the first 20 PCA modes of the digit images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "# fetch MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "# Convert the data and labels into numpy arrays\n",
    "data = np.array(mnist['data'])\n",
    "labels = np.array(mnist['target'])\n",
    "\n",
    "# apply PCA transformation onto the first 20 modes\n",
    "pca = PCA(n_components=20)\n",
    "\n",
    "print(np.shape(data))\n",
    "data_pca_1 = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 20)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(np.shape(data_pca_1))\n",
    "print(np.shape(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### (ii) Build a feed-forward neural network to classify the digits. Compare the results of the neural network against LSTM, SVM (support vector machines) and decision tree classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate training and test data for use in LSTM, SVM, and DTC classifiers\n",
    "data_train, data_test, label_train, label_test = train_test_split(data_pca_1, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# convert labels to ints\n",
    "label_train = label_train.astype(np.int16)\n",
    "label_test = label_test.astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing neural network on MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [0/60000 (0%)]\tLoss: 2.305557\n",
      "Epoch 0 [12800/60000 (21%)]\tLoss: 0.335910\n",
      "Epoch 0 [25600/60000 (43%)]\tLoss: 0.158729\n",
      "Epoch 0 [38400/60000 (64%)]\tLoss: 0.261581\n",
      "Epoch 0 [51200/60000 (85%)]\tLoss: 0.232252\n",
      "Epoch 1 [0/60000 (0%)]\tLoss: 0.348636\n",
      "Epoch 1 [12800/60000 (21%)]\tLoss: 0.097472\n",
      "Epoch 1 [25600/60000 (43%)]\tLoss: 0.189840\n",
      "Epoch 1 [38400/60000 (64%)]\tLoss: 0.082431\n",
      "Epoch 1 [51200/60000 (85%)]\tLoss: 0.073771\n",
      "Epoch 2 [0/60000 (0%)]\tLoss: 0.098106\n",
      "Epoch 2 [12800/60000 (21%)]\tLoss: 0.100959\n",
      "Epoch 2 [25600/60000 (43%)]\tLoss: 0.049346\n",
      "Epoch 2 [38400/60000 (64%)]\tLoss: 0.111405\n",
      "Epoch 2 [51200/60000 (85%)]\tLoss: 0.060046\n",
      "Epoch 3 [0/60000 (0%)]\tLoss: 0.111059\n",
      "Epoch 3 [12800/60000 (21%)]\tLoss: 0.065373\n",
      "Epoch 3 [25600/60000 (43%)]\tLoss: 0.032919\n",
      "Epoch 3 [38400/60000 (64%)]\tLoss: 0.064828\n",
      "Epoch 3 [51200/60000 (85%)]\tLoss: 0.144677\n",
      "Epoch 4 [0/60000 (0%)]\tLoss: 0.039818\n",
      "Epoch 4 [12800/60000 (21%)]\tLoss: 0.039781\n",
      "Epoch 4 [25600/60000 (43%)]\tLoss: 0.041260\n",
      "Epoch 4 [38400/60000 (64%)]\tLoss: 0.057356\n",
      "Epoch 4 [51200/60000 (85%)]\tLoss: 0.024935\n",
      "Epoch 5 [0/60000 (0%)]\tLoss: 0.024790\n",
      "Epoch 5 [12800/60000 (21%)]\tLoss: 0.055396\n",
      "Epoch 5 [25600/60000 (43%)]\tLoss: 0.047570\n",
      "Epoch 5 [38400/60000 (64%)]\tLoss: 0.097803\n",
      "Epoch 5 [51200/60000 (85%)]\tLoss: 0.039567\n",
      "Epoch 6 [0/60000 (0%)]\tLoss: 0.005990\n",
      "Epoch 6 [12800/60000 (21%)]\tLoss: 0.011538\n",
      "Epoch 6 [25600/60000 (43%)]\tLoss: 0.020152\n",
      "Epoch 6 [38400/60000 (64%)]\tLoss: 0.054048\n",
      "Epoch 6 [51200/60000 (85%)]\tLoss: 0.020757\n",
      "Epoch 7 [0/60000 (0%)]\tLoss: 0.011884\n",
      "Epoch 7 [12800/60000 (21%)]\tLoss: 0.020882\n",
      "Epoch 7 [25600/60000 (43%)]\tLoss: 0.017495\n",
      "Epoch 7 [38400/60000 (64%)]\tLoss: 0.031688\n",
      "Epoch 7 [51200/60000 (85%)]\tLoss: 0.036981\n",
      "Epoch 8 [0/60000 (0%)]\tLoss: 0.011645\n",
      "Epoch 8 [12800/60000 (21%)]\tLoss: 0.012516\n",
      "Epoch 8 [25600/60000 (43%)]\tLoss: 0.002924\n",
      "Epoch 8 [38400/60000 (64%)]\tLoss: 0.010089\n",
      "Epoch 8 [51200/60000 (85%)]\tLoss: 0.017719\n",
      "Epoch 9 [0/60000 (0%)]\tLoss: 0.003354\n",
      "Epoch 9 [12800/60000 (21%)]\tLoss: 0.010128\n",
      "Epoch 9 [25600/60000 (43%)]\tLoss: 0.014671\n",
      "Epoch 9 [38400/60000 (64%)]\tLoss: 0.039812\n",
      "Epoch 9 [51200/60000 (85%)]\tLoss: 0.004708\n",
      "Test set: Average loss: 0.0006, Accuracy: 9775/10000 (98%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Download and prepare the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Create data loaders for the training and testing datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model architecture\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = FeedforwardNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "    test_loss, correct, len(test_loader.dataset), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing LSTM on MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [0/60000 (0%)]\tLoss: 2.302297\n",
      "Epoch 0 [12800/60000 (21%)]\tLoss: 0.834746\n",
      "Epoch 0 [25600/60000 (43%)]\tLoss: 0.466113\n",
      "Epoch 0 [38400/60000 (64%)]\tLoss: 0.225567\n",
      "Epoch 0 [51200/60000 (85%)]\tLoss: 0.221007\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[128, 28, 28]' is invalid for input of size 75264",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [184]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     42\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 43\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     45\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(output, target)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[128, 28, 28]' is invalid for input of size 75264"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Download and prepare the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Create data loaders for the training and testing datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the LSTM architecture\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=28, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        out, (h_n, c_n) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = LSTM()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(batch_size, 28, 28)\n",
    "        output = model(data)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data = data.view(data.shape[0], 28, 28)\n",
    "        output = model(data)\n",
    "        test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "    test_loss, correct, len(test_loader.dataset), accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fitting an SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVM: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "# Train a linear classifier\n",
    "clf = SVC()\n",
    "clf.fit(data_train, label_train)\n",
    "\n",
    "# Evaluate the performance on the test set\n",
    "y_pred = clf.predict(data_test)\n",
    "acc = accuracy_score(label_test, y_pred)\n",
    "print(f\"Accuracy for SVM: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fitting a DTC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for DTC: 0.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train a DTC classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(data_train, label_train)\n",
    "\n",
    "# Evaluate the performance on the test set\n",
    "y_pred = clf.predict(data_test)\n",
    "acc = accuracy_score(label_test, y_pred)\n",
    "print(f\"Accuracy for DTC: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06390d54a2cfa057accc1ac5647b8affe2f4673ad9050adec63e83612d831ba8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}